{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required libs and Pulling in Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the dataset\n",
    "dataset_name = \"c4\"\n",
    "task_name = \"en\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"validation\",streaming=True)\n",
    "\n",
    "#convert dataset to a list\n",
    "dataset_list = list(dataset)\n",
    "\n",
    "# count = 0\n",
    "# for example in dataset_list:\n",
    "#     if count >= 1000:\n",
    "#         break\n",
    "#     # Process or use the example as needed\n",
    "#     print(example[\"text\"])  # Or perform any other operation\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting code up to get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sampleing subset for demonstration purposes\n",
    "subset_size = 100\n",
    "data_subset = dataset_list[:min(subset_size, len(dataset_list))]\n",
    "\n",
    "# extract texts from the subset\n",
    "texts = [example[\"text\"] for example in data_subset]\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "# tokenize the texts\n",
    "inputs = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 31s 6s/step\n"
     ]
    }
   ],
   "source": [
    "inputs_tuple = tuple(inputs[key] for key in inputs.keys())\n",
    "predictions = model.predict(inputs_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "logits = predictions.logits\n",
    "\n",
    "# Calculate the mean cross-entropy loss as a measure of perplexity\n",
    "loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(inputs['input_ids'], logits, from_logits=True))\n",
    "perplexity = tf.exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (Loss and Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.500106692314148\n",
      "perplexity: 4.4821672439575195\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss: {loss.numpy()}\")\n",
    "print(f\"perplexity: {perplexity.numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis477",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
