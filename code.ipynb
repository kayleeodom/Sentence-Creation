{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model (loaded from Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kodom\\anaconda3\\envs\\cis477\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import streamlit as st\n",
    "import torch \n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for fine tuning (loaded from Hugging Face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"c4\"\n",
    "task_name = \"en\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"validation\",streaming=True)\n",
    "\n",
    "# count = 0\n",
    "# for example in dataset:\n",
    "#     if count >= 1000:\n",
    "#         break\n",
    "#     # Process or use the example as needed\n",
    "#     print(example[\"text\"])  # Or perform any other operation\n",
    "#     count += 1\n",
    "\n",
    "#convert dataset to a list so I can fine tune\n",
    "dataset_list = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleing subset for demonstration purposes\n",
    "subset_size = 1000\n",
    "data_subset = dataset_list[:min(subset_size, len(dataset_list))]\n",
    "\n",
    "# extracting texts from the subset\n",
    "texts = [example[\"text\"] for example in data_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize / Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the texts\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class MLMCustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.tokenized_texts['input_ids'][index].clone()\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Randomly mask 15% of tokens for MLM training\n",
    "        probability_matrix = torch.rand(input_ids.shape)\n",
    "        mask_indices = probability_matrix < 0.15\n",
    "        mask_indices[torch.logical_or(input_ids == 101, input_ids == 102)] = False  # Do not mask [CLS] and [SEP] tokens\n",
    "        input_ids[mask_indices] = 103  # Mask token ID\n",
    "        \n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': self.tokenized_texts['attention_mask'][index],\n",
    "                'labels': labels}\n",
    "    \n",
    "mlm_dataset = MLMCustomDataset(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(mlm_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting the data into training and validation sets\n",
    "train_dataset = torch.utils.data.Subset(mlm_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(mlm_dataset,val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output/fine_tuned_model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [7:50:31<00:00, 94.10s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28231.4243, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.011, 'train_loss': 0.4919365437825521, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.4919365437825521, metrics={'train_runtime': 28231.4243, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.011, 'train_loss': 0.4919365437825521, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:49<00:00, 11.58s/it]\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19920219480991364, 'eval_runtime': 310.3496, 'eval_samples_per_second': 0.644, 'eval_steps_per_second': 0.081, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presenting Info using streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title(\"Machine Learning Model Presentation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis477",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
